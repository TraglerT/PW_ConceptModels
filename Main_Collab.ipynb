{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9vFEyxS2uez-",
    "outputId": "4d312d3a-9a08-415b-d31c-efb8b47b6d60"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "!pip install wandb -qU\n",
    "import sys\n",
    "sys.path.append(\"/content/drive/MyDrive/ImportScripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZeAONHfv2u5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import derm7pt_data\n",
    "from derm7pt_data import Derm7pt_data\n",
    "from Model import Simple_CNN_Net, Simple_CNN_PerfectConcepts, Concept_To_Label_Net\n",
    "\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GGVy4ByCWta1",
    "outputId": "2d3c63ba-393f-4fbc-ac3d-539532541cfb"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: W&B API key is configured. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "wandb.login(key=\"02e7328f3dec3b552cb764d0b265fbe0a90757a7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FBCeqwuuI57"
   },
   "outputs": [],
   "source": [
    "reload(derm7pt_data)\n",
    "#reload(Model)\n",
    "\n",
    "#Data loading\n",
    "random_state = 42\n",
    "torch.manual_seed(random_state)\n",
    "path = os.path.normpath(\"/content/drive/MyDrive/Derm7pt/\")  #local path: os.path.normpath('Data//Derm7pt')\n",
    "\n",
    "derm7pt = Derm7pt_data(path)\n",
    "metadata = derm7pt.metadata\n",
    "print(\"Data shape:\", metadata.shape)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mILwYIz6dtW5",
    "outputId": "4749c45e-4ce7-46d8-8ca2-20aaeda0f1cb"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['case_num', 'diagnosis', 'seven_point_score', 'pigment_network',\n",
       "       'streaks', 'pigmentation', 'regression_structures', 'dots_and_globules',\n",
       "       'blue_whitish_veil', 'vascular_structures',\n",
       "       'level_of_diagnostic_difficulty', 'elevation', 'location', 'sex',\n",
       "       'clinic', 'derm', 'diagnosis_num', 'is_cancer', 'abbrevs', 'info',\n",
       "       'pigment_network_num', 'pigment_network_score', 'streaks_num',\n",
       "       'streaks_score', 'pigmentation_num', 'pigmentation_score',\n",
       "       'regression_structures_num', 'regression_structures_score',\n",
       "       'dots_and_globules_num', 'dots_and_globules_score',\n",
       "       'blue_whitish_veil_num', 'blue_whitish_veil_score',\n",
       "       'vascular_structures_num', 'vascular_structures_score'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "metadata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKGVT6JgduS0"
   },
   "outputs": [],
   "source": [
    "#Help functions to calculate the majority class baseline\n",
    "def majority_class_baseline(val_idx, mode_txt=\"\"):\n",
    "    print(\"start \", mode_txt, \" baseline: \", datetime.now())\n",
    "    majority_loader = DataLoader(\n",
    "        dataset=derm7pt,\n",
    "        batch_size=999999,\n",
    "        sampler=torch.utils.data.SubsetRandomSampler(val_idx),\n",
    "    )\n",
    "    for i, batch in enumerate(majority_loader, 0):\n",
    "        inputs, labels, concept_labels = batch\n",
    "        baseline, baseline_accuracy = majority_class_accuracy_by_labels(labels)\n",
    "\n",
    "        #concept baseline\n",
    "        concept_baseline = 0\n",
    "        concept_outputs = torch.zeros(len(labels), num_concepts)\n",
    "        concept_baseline_accuracy = ((concept_outputs == concept_labels).sum().item()) / (len(labels)*num_concepts)\n",
    "\n",
    "        print(\"end \", mode_txt, \" baseline:   \", datetime.now(), \", baseline: \", baseline, \" percent \",  baseline_accuracy, \" concept_baseline: \", concept_baseline, \" concept_\", mode_txt, \"_baseline: \", concept_baseline_accuracy)\n",
    "        return baseline_accuracy, concept_baseline_accuracy\n",
    "\n",
    "def majority_class_accuracy_by_labels(true_labels):\n",
    "    # Find the most frequent class in the training set\n",
    "    elems, counts = true_labels.unique(return_counts=True)\n",
    "    majority_count = counts[counts.argmax()]\n",
    "    majority_class = elems[counts.argmax()]\n",
    "    #predictions = torch.full_like(true_labels, majority_class)\n",
    "    accuracy = majority_count / len(true_labels)\n",
    "    return majority_class, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "id": "IbXd1kpxdv27",
    "outputId": "7f305949-669c-4727-defd-3ad3b91980bd"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mtraglert\u001B[0m (\u001B[33mnlp_ass3\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20241106_164945-w3978gxi</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp_ass3/PracticalWork/runs/w3978gxi' target=\"_blank\">run2024-11-06 16:49:45.111596</a></strong> to <a href='https://wandb.ai/nlp_ass3/PracticalWork' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp_ass3/PracticalWork' target=\"_blank\">https://wandb.ai/nlp_ass3/PracticalWork</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp_ass3/PracticalWork/runs/w3978gxi' target=\"_blank\">https://wandb.ai/nlp_ass3/PracticalWork/runs/w3978gxi</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/nlp_ass3/PracticalWork/runs/w3978gxi?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7d882de94550>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "#Training the model\n",
    "# hyperparameters\n",
    "n_epochs = 50\n",
    "learning_rate = 0.0002\n",
    "n_folds = 8\n",
    "batch_size = 8\n",
    "learn_concepts = True   #Defines if loss should be calculated for concepts\n",
    "\n",
    "num_classes = derm7pt.diagnosis_mapping[derm7pt.model_columns[\"label\"]].nunique()\n",
    "num_concepts = len(derm7pt.concepts_mapping)\n",
    "criterion_concept = nn.BCELoss()\n",
    "criterion = nn.CrossEntropyLoss() #Categorical crossEntropyLoss\n",
    "\n",
    "#split up init form main training loop, because of faulty display of print statements during training\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project= \"PracticalWork\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"architecture\": \"Simple_CNN_Net\",\n",
    "    \"dataset\": \"derm7pt\",\n",
    "    \"labels\": derm7pt.model_columns[\"label\"],\n",
    "    \"epochs\": n_epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"n_folds\": n_folds,\n",
    "    \"device\": device,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"num_concepts\": num_concepts,\n",
    "    \"learn_concepts\": learn_concepts,\n",
    "    \"random_state\": random_state,\n",
    "    },\n",
    "    name=\"run\"+str(datetime.now())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "id": "U9GVhFO0dydH",
    "outputId": "019f024e-c411-4555-fecf-0fd88a5e34c9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "start  validation  baseline:  2024-11-06 16:49:48.506588\n",
      "end  validation  baseline:    2024-11-06 16:51:39.466695 , baseline:  tensor(1)  percent  tensor(0.5984)  concept_baseline:  0  concept_ validation _baseline:  0.7716535433070866\n",
      "[1,  16] loss: 1.5412, val_accuracy: 0.5984, time: 2024-11-06 17:04:13\n",
      "[2,  16] loss: 1.5110, val_accuracy: 0.5984, time: 2024-11-06 17:04:35\n",
      "[3,  16] loss: 1.4903, val_accuracy: 0.5984, time: 2024-11-06 17:04:55\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-e19e0b84578c>\u001B[0m in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     29\u001B[0m         \u001B[0mtrain_concepts_total_correct\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     30\u001B[0m         \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 31\u001B[0;31m         \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     32\u001B[0m             \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconcept_labels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     33\u001B[0m             \u001B[0;31m#one hot encoding of the label\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    699\u001B[0m                 \u001B[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    700\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[call-arg]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 701\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    702\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_num_yielded\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    703\u001B[0m             if (\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m_next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    755\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_next_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    756\u001B[0m         \u001B[0mindex\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# may raise StopIteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 757\u001B[0;31m         \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_fetcher\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# may raise StopIteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    758\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_pin_memory\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    759\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_pin_memory_device\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001B[0m in \u001B[0;36mfetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     50\u001B[0m                 \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__getitems__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     51\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 52\u001B[0;31m                 \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     53\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     54\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     50\u001B[0m                 \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__getitems__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     51\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 52\u001B[0;31m                 \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     53\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     54\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/content/drive/MyDrive/ImportScripts/derm7pt_data.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m     86\u001B[0m         \u001B[0mmetadata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmetadata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0miloc\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     87\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 88\u001B[0;31m         \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloadImage\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmetadata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'derm'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     89\u001B[0m         \u001B[0;31m# Convert the Pandas Series to a NumPy array and then to a PyTorch tensor\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     90\u001B[0m         \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/content/drive/MyDrive/ImportScripts/derm7pt_data.py\u001B[0m in \u001B[0;36mloadImage\u001B[0;34m(self, file)\u001B[0m\n\u001B[1;32m    105\u001B[0m         \u001B[0mfile\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnormpath\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfile\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    106\u001B[0m         \u001B[0mpath\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimage_folder\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfile\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 107\u001B[0;31m         \u001B[0mimg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mImage\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__find_case_insensitive_path\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    108\u001B[0m         \u001B[0mimg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mimg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimage_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    109\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mimg\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001B[0m in \u001B[0;36mopen\u001B[0;34m(fp, mode, formats)\u001B[0m\n\u001B[1;32m   3426\u001B[0m     \u001B[0mfilename\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m \u001B[0;34m|\u001B[0m \u001B[0mbytes\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3427\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mis_path\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3428\u001B[0;31m         \u001B[0mfilename\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrealpath\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfspath\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3429\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3430\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mfilename\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.10/posixpath.py\u001B[0m in \u001B[0;36mrealpath\u001B[0;34m(filename, strict)\u001B[0m\n\u001B[1;32m    394\u001B[0m symbolic links encountered in the path.\"\"\"\n\u001B[1;32m    395\u001B[0m     \u001B[0mfilename\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfspath\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 396\u001B[0;31m     \u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mok\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_joinrealpath\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfilename\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstrict\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    397\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mabspath\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    398\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.10/posixpath.py\u001B[0m in \u001B[0;36m_joinrealpath\u001B[0;34m(path, rest, strict, seen)\u001B[0m\n\u001B[1;32m    429\u001B[0m         \u001B[0mnewpath\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    430\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 431\u001B[0;31m             \u001B[0mst\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlstat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnewpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    432\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mOSError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    433\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mstrict\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(derm7pt.metadata)):\n",
    "    #get the majority class of the validation and test set\n",
    "    simple_val_baseline, concept_val_baseline = majority_class_baseline(val_idx, \"validation\")\n",
    "    #left out for performance reasons simple_train_baseline, concept_train_baseline = majority_class_baseline(train_idx, \"train\")\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=derm7pt,\n",
    "        batch_size=batch_size,\n",
    "        sampler=torch.utils.data.SubsetRandomSampler(train_idx),\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        dataset=derm7pt,\n",
    "        batch_size=batch_size,\n",
    "        sampler=torch.utils.data.SubsetRandomSampler(val_idx),\n",
    "    )\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = Simple_CNN_Net(num_classes=num_classes,num_concepts=num_concepts, image_size=derm7pt.image_size)\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0.0\n",
    "        running_loss_concepts = 0.0\n",
    "        i = 0\n",
    "        train_total_correct = 0\n",
    "        train_concepts_total_correct = 0\n",
    "        model.train()\n",
    "        for i, batch in enumerate(train_loader, 0):\n",
    "            inputs, labels, concept_labels = batch\n",
    "            #one hot encoding of the label\n",
    "            hot_labels = torch.eye(num_classes)[labels.squeeze().int()]\n",
    "            inputs, hot_labels, concept_labels = inputs.to(device), hot_labels.to(device), concept_labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass for both concepts and outputs\n",
    "            concept_outputs, outputs = model(inputs)\n",
    "            if learn_concepts:\n",
    "                loss_concepts = criterion_concept(concept_outputs, concept_labels)\n",
    "                loss_concepts.backward(retain_graph=True)\n",
    "                # statistics: average loss\n",
    "                running_loss_concepts += loss_concepts.item()\n",
    "\n",
    "                # concept accuracy\n",
    "                train_concepts_total_correct += (concept_outputs.round() == concept_labels).sum().item()\n",
    "            loss_outputs = criterion(outputs, hot_labels)\n",
    "            loss_outputs.backward()\n",
    "            optimizer.step()\n",
    "            # statistics: average loss\n",
    "            running_loss += loss_outputs.item()\n",
    "\n",
    "            # train accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total_correct += (predicted == hot_labels.argmax(dim=1)).sum().item()\n",
    "            #if i%80 == 0:\n",
    "                #print(\"i \", i, predicted, \"\\n\", labels, \"\\n\", outputs)\n",
    "\n",
    "\n",
    "        running_loss /= (i+1)\n",
    "        running_loss_concepts /= (i+1)\n",
    "        train_accuracy = train_total_correct / len(train_idx)\n",
    "        concept_train_accuracy = train_concepts_total_correct / (len(train_idx)*num_concepts)\n",
    "\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        concept_correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(val_loader, 0):\n",
    "                inputs, labels, concept_labels = batch\n",
    "                inputs, labels, concept_labels = inputs.to(device), labels.to(device), concept_labels.to(device)\n",
    "                concept_outputs, outputs = model(inputs)\n",
    "                outputs = outputs.argmax(dim=1)\n",
    "                total += labels.size(0)\n",
    "                correct += (outputs == labels).sum().item()\n",
    "                concept_correct += (concept_outputs.round() == concept_labels).sum().item()\n",
    "        val_accuracy = correct/total\n",
    "        concept_val_accuracy = concept_correct/(total*num_concepts)\n",
    "        wandb.log({\"loss\": running_loss, \"train_accuracy\": train_accuracy, \"concept_loss:\": running_loss_concepts, \"concept_train_accuracy\": concept_train_accuracy, \"validation_accuracy\": val_accuracy, \"concept_validation_accuracy\": concept_val_accuracy})\n",
    "        print('[%d, %3d] loss: %.4f, val_accuracy: %.4f, time: %s' % (epoch + 1, i + 1, running_loss, val_accuracy, datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "\n",
    "    #Only one fold for performance reasons\n",
    "    break\n",
    "\n",
    "wandb.finish()\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
